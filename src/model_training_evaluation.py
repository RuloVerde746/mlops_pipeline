# -*- coding: utf-8 -*-
"""
model_training_evaluation.py - Entrenamiento y Evaluaci√≥n de Modelos

Basado en el notebook de referencia pero adaptado para usar ft_engineering.py mejorado
Implementa el flujo completo: carga de datos ‚Üí entrenamiento ‚Üí evaluaci√≥n ‚Üí guardado
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import warnings
warnings.filterwarnings('ignore')

# Importar nuestro m√≥dulo de feature engineering mejorado
from ft_engineering import load_and_prepare_data, summarize_classification_robust

# Importar modelos de sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import xgboost as xgb

# Configuraci√≥n de visualizaciones
plt.style.use('default')
sns.set_style("whitegrid")

def define_models():
    """
    Define los modelos a entrenar (basado en el notebook de referencia)
    
    Esta funci√≥n crea un diccionario con 5 modelos de clasificaci√≥n:
    - Logistic Regression: Modelo lineal b√°sico para clasificaci√≥n binaria
    - SVC: Support Vector Classifier con kernel RBF por defecto
    - Decision Tree: √Årbol de decisi√≥n simple, interpretable
    - Random Forest: Ensemble de √°rboles, reduce overfitting
    - XGBoost: Gradient boosting, alto rendimiento en datos tabulares
    
    Todos los modelos tienen:
    - random_state=42: Para reproducibilidad
    - class_weight='balanced': Para manejar desbalance de clases
    """
    models = {
        "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),
        "SVC": SVC(probability=True, random_state=42, class_weight='balanced'),
        "Decision Tree": DecisionTreeClassifier(random_state=42, class_weight='balanced'),
        "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),
        "XGBoost": xgb.XGBClassifier(eval_metric='logloss', random_state=42, scale_pos_weight=1)
    }
    
    print("ü§ñ Modelos configurados:")
    for name in models.keys():
        print(f"  ‚Ä¢ {name}")
    
    return models

def train_and_evaluate_models(models, X_train, X_test, y_train, y_test):
    """
    Entrena y eval√∫a todos los modelos (basado en el notebook de referencia)
    
    Proceso para cada modelo:
    1. Entrenamiento con .fit() usando datos de entrenamiento
    2. Predicci√≥n con .predict() sobre datos de prueba
    3. C√°lculo de m√©tricas: Accuracy, Precision, Recall, F1-score
    4. Generaci√≥n de matriz de confusi√≥n para an√°lisis de errores
    5. Visualizaci√≥n inmediata de resultados
    
    Maneja excepciones para continuar con otros modelos si alguno falla
    """
    print("\nüöÄ Iniciando entrenamiento y evaluaci√≥n de modelos")
    print("="*60)
    
    resultados = []
    
    for nombre, modelo in models.items():
        print(f"\nüìä Entrenando modelo: {nombre}...")
        
        try:
            # Entrenar modelo con los datos preprocesados
            # X_train: caracter√≠sticas escaladas y codificadas
            # y_train: variable objetivo (0/1 para Pago_atiempo)
            modelo.fit(X_train, y_train)
            
            # Realizar predicciones sobre el conjunto de prueba
            # X_test: datos nunca vistos por el modelo
            y_pred = modelo.predict(X_test)
            
            # Calcular m√©tricas de evaluaci√≥n
            # accuracy: proporci√≥n de predicciones correctas
            # precision: proporci√≥n de positivos predichos que son correctos
            # recall: proporci√≥n de positivos reales que fueron detectados
            # f1: media arm√≥nica de precision y recall
            acc = accuracy_score(y_test, y_pred)
            prec = precision_score(y_test, y_pred, zero_division=0)
            rec = recall_score(y_test, y_pred, zero_division=0)
            f1 = f1_score(y_test, y_pred, zero_division=0)
            
            # Guardar resultados en lista para comparaci√≥n posterior
            resultados.append({
                "Modelo": nombre,
                "Accuracy": acc,
                "Precision": prec,
                "Recall": rec,
                "F1-score": f1
            })
            
            print(f"‚úÖ {nombre} - Accuracy: {acc:.4f}, F1: {f1:.4f}")
            
            # Crear matriz de confusi√≥n para an√°lisis detallado
            # Muestra: TP, FP, FN, TN para entender errores del modelo
            cm = confusion_matrix(y_test, y_pred)
            plt.figure(figsize=(4,3))
            sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
            plt.title(f"Matriz de confusi√≥n - {nombre}")
            plt.xlabel("Predicted")  # Predicciones del modelo
            plt.ylabel("Actual")      # Valores reales
            plt.tight_layout()
            plt.show()
            
        except Exception as e:
            print(f"‚ùå Error entrenando {nombre}: {e}")
            continue
    
    return resultados, models

def compare_models(resultados):
    """
    Compara los resultados de los modelos (basado en el notebook de referencia)
    
    Esta funci√≥n:
    1. Convierte la lista de resultados a DataFrame de pandas
    2. Muestra tabla comparativa con todas las m√©tricas
    3. Transforma datos a formato largo para visualizaci√≥n
    4. Crea gr√°fico de barras comparativo por modelo y m√©trica
    
    El gr√°fico permite identificar visualmente:
    - Qu√© modelo tiene mejor accuracy
    - Cu√°l tiene mejor precision (menos falsos positivos)
    - Cu√°l tiene mejor recall (menos falsos negativos)
    - Cu√°l tiene mejor F1-score (balance precision-recall)
    """
    print(f"\nüìà TABLA COMPARATIVA DE MODELOS")
    print("="*60)
    
    # Convertir lista de diccionarios a DataFrame para mejor visualizaci√≥n
    df_resultados = pd.DataFrame(resultados)
    print(df_resultados.round(4))
    
    # Preparar datos para gr√°fico comparativo
    # melt() transforma de formato ancho a largo para seaborn
    # Ej: [Modelo, Accuracy, Precision, Recall, F1] ‚Üí [Modelo, M√©trica, Valor]
    print(f"\nüìä Generando gr√°fico comparativo...")
    
    df_resultados_melted = df_resultados.melt(id_vars="Modelo", var_name="M√©trica", value_name="Valor")
    
    # Crear gr√°fico de barras comparativo
    plt.figure(figsize=(12,6))
    sns.barplot(x="Modelo", y="Valor", hue="M√©trica", data=df_resultados_melted)
    plt.title("Comparaci√≥n de m√©tricas por modelo", fontsize=14, fontweight='bold')
    plt.ylim(0,1)  # Las m√©tricas est√°n entre 0 y 1
    plt.xticks(rotation=45)  # Rotar etiquetas para mejor legibilidad
    plt.legend(loc="lower right")  # Ubicar leyenda donde no tape datos
    plt.grid(True, alpha=0.3)  # Cuadr√≠cula sutil para facilitar lectura
    plt.tight_layout()  # Ajustar para que no se corten etiquetas
    plt.show()
    
    return df_resultados

def select_best_model(df_resultados, models):
    """
    Selecciona el mejor modelo basado en F1-score
    
    El F1-score es elegido porque:
    - Es la media arm√≥nica de precision y recall
    - Penaliza modelos con performance desbalanceada
    - Es ideal para problemas con clases desbalanceadas como el nuestro
    
    Proceso:
    1. Encontrar √≠ndice del m√°ximo F1-score
    2. Obtener nombre y m√©tricas del mejor modelo
    3. Recuperar objeto del modelo entrenado
    4. Mostrar resumen detallado del ganador
    """
    print(f"\nüèÜ SELECCI√ìN DEL MEJOR MODELO")
    print("="*40)
    
    # Encontrar el mejor modelo por F1-score
    # idxmax() devuelve el √≠ndice del valor m√°ximo en la columna
    best_idx = df_resultados['F1-score'].idxmax()
    best_model_name = df_resultados.loc[best_idx, 'Modelo']
    best_f1 = df_resultados.loc[best_idx, 'F1-score']
    
    print(f"ü•á Mejor modelo: {best_model_name}")
    print(f"üìà F1-score: {best_f1:.4f}")
    
    # Mostrar todas las m√©tricas del mejor modelo
    best_metrics = df_resultados.loc[best_idx]
    print(f"\nüìä M√©tricas completas:")
    for metric, value in best_metrics.items():
        if metric != 'Modelo':
            print(f"  ‚Ä¢ {metric}: {value:.4f}")
    
    # Obtener el objeto del mejor modelo del diccionario original
    best_model = models.get(best_model_name)
    
    return best_model, best_model_name, best_metrics

def save_model_and_artifacts(best_model, best_model_name, preprocessor, X_train):
    """
    Guarda el mejor modelo y artefactos necesarios (basado en el notebook de referencia)
    
    Archivos generados:
    1. Modelo entrenado (.pkl) - Para predicciones en producci√≥n
    2. Preprocesador (.pkl) - Para transformar nuevos datos
    3. Datos referencia (.csv) - Base para monitoreo PSI
    4. Metadatos (.pkl) - Informaci√≥n del modelo para trazabilidad
    
    joblib se usa porque:
    - Es m√°s eficiente que pickle para objetos de sklearn
    - Mantiene compatibilidad entre versiones
    - Permite guardar objetos complejos como pipelines
    """
    import os
    
    print(f"\nüíæ GUARDANDO MODELO Y ARTEFACTOS")
    print("="*45)
    
    # Obtener directorio ra√≠z del proyecto
    script_dir = os.path.dirname(__file__)
    root_dir = os.path.abspath(os.path.join(script_dir, '..'))
    
    # Crear directorio data/processed si no existe
    processed_dir = os.path.join(root_dir, 'data', 'processed')
    os.makedirs(processed_dir, exist_ok=True)
    
    # Guardar el mejor modelo con nombre din√°mico
    # El nombre incluye el tipo de modelo para identificaci√≥n clara
    model_filename = f'mejor_modelo_{best_model_name.lower().replace(" ", "_")}.pkl'
    model_path = os.path.join(root_dir, model_filename)
    joblib.dump(best_model, model_path)
    print(f"‚úÖ Modelo guardado como: {model_filename}")
    
    # Guardar el preprocesador (pipeline de transformaci√≥n)
    # Esencial para aplicar las mismas transformaciones a datos nuevos
    preprocessor_path = os.path.join(root_dir, 'preprocesador.pkl')
    joblib.dump(preprocessor, preprocessor_path)
    print("‚úÖ Preprocesador guardado como: preprocesador.pkl")
    
    # Guardar X_train para referencia PSI (Population Stability Index)
    # PSI mide si la distribuci√≥n de datos cambia en producci√≥n
    # Los datos de entrenamiento son la l√≠nea base
    X_train_df = pd.DataFrame(X_train)
    reference_data_path = os.path.join(processed_dir, 'data_referencia.csv')
    X_train_df.to_csv(reference_data_path, index=False)
    print(f"‚úÖ Datos de referencia guardados como: data/processed/data_referencia.csv")
    
    # Crear archivo de metadatos para trazabilidad completa
    # Incluye informaci√≥n importante para MLOps y auditor√≠a
    metadata = {
        'best_model': best_model_name,
        'model_file': model_filename,
        'preprocessor_file': 'preprocesador.pkl',
        'reference_data': 'data/processed/data_referencia.csv',
        'creation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
        'purpose': 'Predicci√≥n de pago a tiempo de cr√©ditos',
        'target_variable': 'Pago_atiempo'
    }
    
    metadata_path = os.path.join(root_dir, 'model_metadata.pkl')
    joblib.dump(metadata, metadata_path)
    print("‚úÖ Metadatos guardados como: model_metadata.pkl")

def main():
    """
    Funci√≥n principal que ejecuta el flujo completo de MLOps
    
    Este flujo implementa las mejores pr√°cticas para desarrollo de modelos:
    1. Carga y preparaci√≥n de datos con feature engineering robusto
    2. Definici√≥n de m√∫ltiples modelos para comparaci√≥n
    3. Entrenamiento y evaluaci√≥n sistem√°tica
    4. An√°lisis comparativo visual y num√©rico
    5. Selecci√≥n autom√°tica del mejor modelo
    6. Guardado de artefactos para producci√≥n
    
    Cada fase est√° dise√±ada para ser:
    - Reproducible (mismos random_state)
    - Escalable (funciona con diferentes datasets)
    - Auditable (logs detallados y metadatos)
    - Robusta (manejo de excepciones)
    """
    print("üéØ INICIANDO ENTRENAMIENTO Y EVALUACI√ìN DE MODELOS")
    print("="*65)
    
    # 1. Cargar y preparar datos usando ft_engineering.py mejorado
    # Esta funci√≥n aplica: feature engineering, encoding, scaling, train-test split
    print("\nüìä FASE 1: CARGA Y PREPARACI√ìN DE DATOS")
    print("-"*50)
    
    pipeline_result = load_and_prepare_data()
    
    if pipeline_result is None:
        print("‚ùå Error: No se pudieron cargar los datos")
        return
    
    # Extraer componentes del pipeline
    X_train = pipeline_result['X_train']      # Caracter√≠sticas para entrenamiento
    X_test = pipeline_result['X_test']        # Caracter√≠sticas para prueba
    y_train = pipeline_result['y_train']      # Variable objetivo entrenamiento
    y_test = pipeline_result['y_test']        # Variable objetivo prueba
    preprocessor = pipeline_result['preprocessor']  # Pipeline de transformaci√≥n
    
    print(f"‚úÖ Datos cargados: X_train={X_train.shape}, X_test={X_test.shape}")
    
    # 2. Definir modelos para competici√≥n
    # Cada modelo tiene diferentes fortalezas para encontrar el mejor
    print("\nü§ñ FASE 2: DEFINICI√ìN DE MODELOS")
    print("-"*50)
    
    models = define_models()
    
    # 3. Entrenar y evaluar todos los modelos
    # Esta es la fase principal de machine learning
    print("\nüöÄ FASE 3: ENTRENAMIENTO Y EVALUACI√ìN")
    print("-"*50)
    
    resultados, trained_models = train_and_evaluate_models(
        models, X_train, X_test, y_train, y_test
    )
    
    # 4. Comparar resultados visualmente
    # Permite identificar patrones y seleccionar el mejor modelo
    print("\nüìà FASE 4: COMPARACI√ìN DE MODELOS")
    print("-"*50)
    
    df_resultados = compare_models(resultados)
    
    # 5. Seleccionar autom√°ticamente el mejor modelo
    # Basado en F1-score para balancear precision y recall
    print("\nüèÜ FASE 5: SELECCI√ìN DEL MEJOR MODELO")
    print("-"*50)
    
    best_model, best_model_name, best_metrics = select_best_model(
        df_resultados, trained_models
    )
    
    # 6. Guardar modelo y artefactos para producci√≥n
    # Essential para MLOps y deployment
    print("\nüíæ FASE 6: GUARDADO DE ARTEFACTOS")
    print("-"*50)
    
    save_model_and_artifacts(best_model, best_model_name, preprocessor, X_train)
    
    # 7. Resumen final del proceso completo
    # Proporciona visibilidad del √©xito y archivos generados
    print(f"\nüéâ PROCESO COMPLETADO EXITOSAMENTE!")
    print("="*50)
    print(f"üìä Resumen final:")
    print(f"  ‚Ä¢ Modelos evaluados: {len(resultados)}")
    print(f"  ‚Ä¢ Mejor modelo: {best_model_name}")
    print(f"  ‚Ä¢ F1-score: {best_metrics['F1-score']:.4f}")
    print(f"  ‚Ä¢ Accuracy: {best_metrics['Accuracy']:.4f}")
    print(f"  ‚Ä¢ Archivos generados:")
    print(f"    - Modelo: mejor_modelo_{best_model_name.lower().replace(' ', '_')}.pkl")
    print(f"    - Preprocesador: preprocesador.pkl")
    print(f"    - Referencia: data_referencia.csv")
    print(f"    - Metadatos: model_metadata.pkl")
    
    # Retornar diccionario con todos los resultados para uso posterior
    return {
        'results': df_resultados,
        'best_model': best_model,
        'best_model_name': best_model_name,
        'best_metrics': best_metrics,
        'pipeline': pipeline_result
    }

if __name__ == "__main__":
    results = main()
